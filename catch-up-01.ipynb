{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to Natural Language Processing Catch-up 1\n",
    " \n",
    "Authors :\n",
    " * Tony George"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "*(Copied from subject)*\n",
    "\n",
    "In this small project, you will code a sentiment classifier using the naive Bayes methods seen in class and compare it with the FastText library. There are a few theoritical questions to answer as well.\n",
    "\n",
    "Please, read the full assignment before starting.\n",
    "\n",
    "For coding standards, please respect the following guidelines\n",
    "\n",
    "* Use docstring format to describe your functions and their arguments.\n",
    "* Use typing.\n",
    "* Have clear and verbatim variable names (not x, x1, x2, xx, another_x, ...).\n",
    "* Make your results reproducible (force random seeds values when necessary and possible).\n",
    "* Don't hesitate commenting in details part of the code you consider complex or hard to read.\n",
    "\n",
    "Do not hesitate contacting me if you have any question, but please don't wait until the last moment to do so."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports\n",
    "\n",
    "Using conda with python version 3.8. A conda yml should be available if I didn't forget to generate it.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# TODO : Generate conda yml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Forcing seed\n",
    "\n",
    "This helps reproducibility, feel free to play with !"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The dataset\n",
    "\n",
    "Using HuggingFace's version of the IMDB dataset as asked by subject.\n",
    "\n",
    "### Importing from HuggingFace"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desc : Large Movie Review Dataset.\n",
      "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "Features : {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None)}\n",
      "Splits : {'train': SplitInfo(name='train', num_bytes=33432835, num_examples=25000, dataset_name='imdb'), 'test': SplitInfo(name='test', num_bytes=32650697, num_examples=25000, dataset_name='imdb'), 'unsupervised': SplitInfo(name='unsupervised', num_bytes=67106814, num_examples=50000, dataset_name='imdb')}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_dataset_builder\n",
    "\n",
    "ds_info = load_dataset_builder(\"imdb\")\n",
    "print(\"Desc :\", ds_info.info.description)\n",
    "print(\"Features :\", ds_info.info.features)\n",
    "print(\"Splits :\", ds_info.info.splits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Reusing dataset imdb (C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    }
   ],
   "source": [
    "# Cheating a bit by downloading the DS now\n",
    "ds_train    = load_dataset(\"imdb\", split = \"train\")\n",
    "ds_test     = load_dataset(\"imdb\", split = \"test\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset Exploration\n",
    "\n",
    "#### How many splits does the dataset has?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['train', 'test', 'unsupervised'])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the builder to get info on splits.\n",
    "ds_info.info.splits.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "HuggingFaces's version of the IMDB dataset has 3 different splits, though we will only interest ourselves in the first two ones.\n",
    "\n",
    "The *train* and *test* split are your standard ML splits, while the *unsupervised* data contains unlabelled data for those willing to gain more volumetry at the cost of some work."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### How big are these splits?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['Split train contains 25000 examples (33.4 MB).',\n 'Split test contains 25000 examples (32.7 MB).',\n 'Split unsupervised contains 50000 examples (67.1 MB).']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This small lib is great to convert hard numbers into a human-readable format.\n",
    "from humanize import naturalsize\n",
    "from datasets import SplitInfo\n",
    "\n",
    "def split_info_on(sp_info : SplitInfo) -> str:\n",
    "    \"\"\"\n",
    "    :param sp_info: The SplitInfo object from HuggingFace's datasets lib.\n",
    "    :return: Human readable string with a quick description on the object.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Split {sp_info.name} contains {sp_info.num_examples} examples ({naturalsize(sp_info.num_bytes)}).\"\"\"\n",
    "\n",
    "# Coming from Scala, having to wrap a map object in Python makes me sad.\n",
    "# You will further down that my mindset is heavily ~~~corrupted~~~ inspired by MapReduce.\n",
    "list(map(split_info_on, ds_info.info.splits.values()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In term of size, we have a 50/50 split between labelled and unlabelled data, and another 50/50 between the *train* and *test* split for the former one."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### What is the proportion of each class on the supervised splits?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from typing import Dict\n",
    "\n",
    "def class_histogram(ds : Dataset) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    :param ds: Dataset object to dress the class histogram of.\n",
    "    :return: List of Tuple2 [('class_label', count), ...]\n",
    "    \"\"\"\n",
    "    labels = ds.info.features['label'].names\n",
    "    # Extracting the label from the example, then counting occurrences using np.unique()\n",
    "    _, counts = np.unique(ds['label'], return_counts = True)\n",
    "    return dict(zip(labels, counts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : {'neg': 12500, 'pos': 12500}\n",
      "Test : {'neg': 12500, 'pos': 12500}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train :\", class_histogram(ds_train))\n",
    "print(\"Test :\", class_histogram(ds_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both our splits have no bias toward a class or the other, with again a perfect  50/50 split."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes classifier\n",
    "\n",
    "### Pretreatment\n",
    "\n",
    "Using the standard NLP pretreatment workflow, up to stemming.\n",
    "\n",
    "The reason I don't do lemmatization is because my pretreatment function is simple and tends to butcher words.\n",
    "(i.e. \"you'll\" becomes [\"you\", \"'ll\"], and wordnet lemmatizer does not recognize the second word as will).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Griffures\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Griffures\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Downloading NLTK (we will use it below anyway).\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stops_en = set(stopwords.words('english'))\n",
    "lancaster = LancasterStemmer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def pretreatment(text : str) -> str:\n",
    "    # Lowercasing\n",
    "    raw_words = word_tokenize(text.lower())\n",
    "\n",
    "    # Filter out : punctuation & stop stopwords\n",
    "    is_relevant = lambda word : not (all(map(lambda c: c in punctuation, list(word))) # Removing punctuation only words\n",
    "                                     or word in stops_en) # BONUS : Removing stopwords\n",
    "    filtered_words = list(filter(is_relevant, raw_words))\n",
    "\n",
    "    # BONUS : Stemming\n",
    "    lemmatized_words = list(map(lancaster.stem, filtered_words))\n",
    "\n",
    "    return  ' '.join(lemmatized_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example with : I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\n",
      "Which gives : went saw movy last night coax friend min 'll admit reluct see knew ashton kutch abl comedy wrong kutch play charact jak fisch wel kevin costn play ben randal profess sign good movy toy emot on exact entir the sold overcom laught first half movy mov tear second half exit the saw many wom tear many ful grown men wel try desp let anyon see cry movy gre suggest go see judg\n"
     ]
    }
   ],
   "source": [
    "# Select an example here\n",
    "ex_preprocessing = ds_test[0]['text']\n",
    "\n",
    "print(\"Example with :\", ex_preprocessing)\n",
    "print(\"Which gives :\", pretreatment(ex_preprocessing))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementing the model\n",
    "\n",
    "As to not rely too much on sklearn's library, and only using it to gain time,\n",
    "we will override the preprocessing function of the CountVectorizer with our own, and disable its stopwords list.\n",
    "\n",
    "The model will actually take a hit from this workflow (both in performance, as the tokenizing will actually happen twice, and in accuracy, as we could do everything with a well configured CountVectorizer).\n",
    "However, it proves a greater comprehension of the notions at play."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy.typing as npt\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class CustomNaiveBayes:\n",
    "    \"\"\"\n",
    "    Wrapper class to articulate sklearn's CountVectorizer and MultinomialNB models.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(preprocessor = pretreatment, stop_words = None)\n",
    "    clf = MultinomialNB()\n",
    "    labels = []\n",
    "\n",
    "    def fit(self, ds : Dataset) -> None:\n",
    "        \"\"\"\n",
    "        Wrapper function to launch a complete train workflow from a HF's dataset\n",
    "        :param ds: HuggingFace DataSet object to train on.\n",
    "        :return: Nothing.\n",
    "        \"\"\"\n",
    "        # Extracting labels for predict_label function\n",
    "        self.labels = ds.features['label'].names\n",
    "        # Shuffling and extracting features\n",
    "        ds.shuffle()\n",
    "        X_as_docs, y_as_ints = self.DS_to_XnY(ds)\n",
    "        X_as_features = self.vectorizer.fit_transform(X_as_docs)\n",
    "        # Actual training\n",
    "        self.clf.fit(X_as_features, y_as_ints)\n",
    "\n",
    "    def predict(self, X_as_docs : npt.ArrayLike) -> npt.NDArray[int]:\n",
    "        \"\"\"\n",
    "        Extract features then launch model's predictions on a list of documents\n",
    "        :param X_as_docs: List of documents (corpus)\n",
    "        :return: Predicted classes as ints\n",
    "        \"\"\"\n",
    "        X_as_features = self.vectorizer.transform(X_as_docs)\n",
    "        return self.clf.predict(X_as_features)\n",
    "\n",
    "    def predict_label(self, X_as_docs : npt.ArrayLike) -> npt.NDArray[str]:\n",
    "        \"\"\"\n",
    "        Same as predict, but return the classes label\n",
    "        :param X_as_docs: List of documents (corpus)\n",
    "        :return: Predicted classes as strings\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X_as_docs)\n",
    "        return np.fromiter([self.labels[y] for y in predictions], str)\n",
    "\n",
    "    def score(self, ds : Dataset) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model with a HF's dataset.\n",
    "        :param ds: HuggingFace DataSet object to evaluate upon.\n",
    "        :return: Accuracy as float (0~1).\n",
    "        \"\"\"\n",
    "        X_as_docs, y_as_ints = self.DS_to_XnY(ds)\n",
    "        X_as_features = self.vectorizer.transform(X_as_docs)\n",
    "        return self.clf.score(X_as_features, y_as_ints)\n",
    "\n",
    "    def DS_to_XnY(self, ds : Dataset) -> Tuple[npt.ArrayLike, npt.ArrayLike]:\n",
    "        \"\"\"\n",
    "        Transforms a HuggingFace dataset object to a more usual X and y tuple for fitting.\n",
    "        :param ds: Dataset to convert\n",
    "        :return: Tuple : X as list[str] and Y as list[int], both of the same len.\n",
    "        \"\"\"\n",
    "        raw_corpus = np.array(ds['text'])\n",
    "        targets = np.array(ds['label'])\n",
    "        return raw_corpus, targets\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a\\cache-9d17e850e5680b7a.arrow\n"
     ]
    }
   ],
   "source": [
    "customNB = CustomNaiveBayes()\n",
    "customNB.fit(ds_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reporting accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.81168\n"
     ]
    }
   ],
   "source": [
    "customNB_acc = customNB.score(ds_test)\n",
    "print(\"Accuracy :\", customNB_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Is accuracy it sufficient here ?\n",
    "\n",
    "Accuracy is a sufficient metric here for multiple reasons :\n",
    "\n",
    "* Since we only have 2 classes, a confusion matrice looses its relevance (p versus 1 - p).\n",
    "* Due to the nature of our classification, we do not care specifically for type 1 nor type 2 errors. Therefore focusing Precision or Recall is meaningless.\n",
    "* The above, plus the fact that our classes are perfectly balanced, makes f1-score a very similar metric as accuracy (harmonic mean between precision and recall)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Top 10 important feature from each class\n",
    "\n",
    "I already removed the stop words, so we already have the best results ! :)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def top_10_features_per_class(model: CustomNaiveBayes) -> Dict:\n",
    "    \"\"\"\n",
    "    Return the top ten heaviest features for each class according to the model\n",
    "    :param model: the model to extract weights from\n",
    "    :return: Dictionnary : Class Label -> Features (feature(stem) -> log_value)\n",
    "    \"\"\"\n",
    "    # Using a custom dtype to preserve info on features' names.\n",
    "    feature_log_t = [(\"feature\", '<U50'), (\"log_value\", float)]\n",
    "    # Just stacks the features names (stems) on top of the logs array\n",
    "    feature_stack = lambda logs : np.array(list(zip(model.vectorizer.get_feature_names_out(), logs)), dtype = feature_log_t)\n",
    "    # Sort the array, take the 10 last value, then flip it to get our top 10\n",
    "    sort_10_biggest = lambda arr : np.flip(np.sort(arr, order=\"log_value\")[-10:])\n",
    "\n",
    "    # Applying the functions in the correct order\n",
    "    logs_with_features = [sort_10_biggest(feature_stack(logs)) for logs in model.clf.feature_log_prob_]\n",
    "\n",
    "    return dict(zip(model.labels, map(dict, logs_with_features)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features (stems) in NEG :\n",
      "  * br        -3.41\n",
      "  * movy      -4.04\n",
      "  * film      -4.25\n",
      "  * on        -4.78\n",
      "  * act       -4.79\n",
      "  * lik       -4.87\n",
      "  * ev        -4.89\n",
      "  * real      -5.09\n",
      "  * mak       -5.27\n",
      "  * bad       -5.3\n",
      "Top 10 features (stems) in POS :\n",
      "  * br        -3.51\n",
      "  * film      -4.18\n",
      "  * movy      -4.32\n",
      "  * on        -4.76\n",
      "  * act       -5.05\n",
      "  * lik       -5.06\n",
      "  * real      -5.09\n",
      "  * ev        -5.2\n",
      "  * tim       -5.31\n",
      "  * good      -5.35\n"
     ]
    }
   ],
   "source": [
    "# Simple extraction of the above dictionary\n",
    "for label, features in top_10_features_per_class(customNB).items():\n",
    "    print(f\"Top 10 features (stems) in {label.upper()} :\")\n",
    "    print('\\n'.join([f\"  * {feature:10}{log:2.3}\" for feature, log in features.items()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysis :\n",
    "\n",
    "Prevalent stems common in both classes (8/10) :\n",
    "* br\n",
    "* movy\n",
    "* film\n",
    "* on\n",
    "* act\n",
    "* like\n",
    "* real\n",
    "* ev\n",
    "\n",
    "Prevalent NEG stems :\n",
    "* mak\n",
    "* bad\n",
    "\n",
    "Prevalent POS stems :\n",
    "* time\n",
    "* good\n",
    "\n",
    "A great number of steams are common between our two classes. This makes me think that a greater notion of context should be given to better the model.\n",
    "Just giving the CountVectorizer 2-grams would help it makes the difference between 'bad' and 'not bad', 'good act' and 'bad act', etc.\n",
    "\n",
    "However 81% for such a simple model is not bad at all already ! (n-grams would explode the size of our vocabulary)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Digging up errors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def get_errors(model : CustomNaiveBayes, ds : Dataset, n):\n",
    "    X_as_docs, Y = model.DS_to_XnY(ds.shuffle()[n * 100:])\n",
    "    y_pred = model.predict(X_as_docs)\n",
    "    errors = X_as_docs[y_pred != Y]\n",
    "    return errors[:n]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a\\cache-e3a051aef336c446.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "array(['(the description of the mood of the movie may be considered as a spoiler - because there is not much action in fact)<br /><br />Great one...<br /><br />Is it for my peculiar interest for the dystopias and utopias? Is it for the atmosphere of the movie. Or is there some more magic? If yes, it is for sure the utmost human one...<br /><br />This film is, no doubt, extremely artistic/artificial (depends on taste). I can imagine most of the people who hate to watch slow movies (and those of Tsai Ming Liang (who I didn\\'t enjoy other times) are one of the slowest that I know), suffering during the movie. Yes, people are unable to slow down and to let time pass - and to watch it without feeling they waste it. One can take this piece as torture or as a therapy...<br /><br />The topic at the surface? The lack of communication - even if we live in rabbit cages - one next to each other - but not really together? People are tired, sick of something and unable to describe it - just don\\'t want to meet, touch, talk, confront the others... like if they had disappeared. The big block of flats looks void and the rain falling constantly evokes the strange melancholy inside. And sometimes it must be something abnormal, unexpected, some unwanted decay as a hole in the floor of concrete - that allows us to reach each other.<br /><br />One of the possible ways to look at it is this: Don\\'t survey the inner world of the characters - consider the whole movie-space to be inside of yourself. And ask - why is it there? Where could these depressive states and moods come form? Is there a place for them, they don\\'t have a right to be here? And search for the answers (if you need them) among the walls and halls of the block - instead of inside hardly transparent mind of a man.<br /><br />The key to understand is not-to-understand - to let a movie borrow us - as a subject of study - inside itself - and at the end safely return us to our more colorful and \"normal\" looking reality.<br /><br />Then, maybe, you will reach - like me - the feeling of real, possible, non-pathetic hope, that in core we are still humans... and this state of mind can help one much to live in this world.',\n       \"The trailer to this film focused so much on the chain (of course, because it's so sensational) that it missed most of the movie, which is about a developing, although rather simply drawn, relationship between Lazarus and Rae as they attempt to recover from their past pains with each other.<br /><br />Of course, with the premise of a nymphomaniac in chains, it's no surprise that there's plenty of implied sex involved. However, at it's core, Black Snake Moan is a basic tale of redemption and the healing power of helping another person along. Maybe it's just me though, but I think poor Lazarus should've had his story focused on more. He's a hurting man after his wife leaves him, but we never fully see how helping Rae resolve her past pains heals him too. It's just implied that it does--in essence, he plays the wizard that helps the young Rae overcome her curse, through a big ol' chain and some blues.<br /><br />I like the story, but I wish it were a bit more even and didn't have to rely on the sensational. The side characters were fairly decent, if simple and I liked the music. The acting was good enough, although I can't be certain if the Rae character is fully believable. But that might just be my naivety.<br /><br />All in all, I liked the film, but I wasn't compelled by it. Maybe it's that I'm too critical, but the story seems a little too convenient to be fully believable and so, while it all seemed very cool, I could never truly buy it. The chain thing was a little too far-fetched for me. Still, this can provide some entertainment for those looking for dramatic redemption stories with a shot of the blues. 7/10\",\n       \"I haven't written a review on here in ages but rewatching all of bottom TV show, live shows and this I felt I had to make my views on this movie known! It is, I feel, the perfect comedy movie. It lacks the lovey dovey story lines(I wouldn't really call richies enfatuation with Gina Carbonara love would you? Or him and eddie going up there naked... not love) that make the rest of comedys go from good to crap, it lacks the usual dilemmas that one must overcome in most other comedy movies... unless you count the fact that they poisoned the guests and must escape from the guests green vomit as a dilemma thats similar to other comedy movies..... No, this movie just sets out and succeeds in doing one thing AND ONE thing only: Making one laugh. What does one require from comedy movies? Laughter. This movie just piles on laugh after laugh without stuffing up the laughs with serious crap like other comedy movies! Thus I call it the, so far, only perfect comedy movie ever made and I will never ever stop watching this beautiful movie! I appluad rick and ade on such fantastic genius!\",\n       '15 PARK AVENUE is the address \"Mithi/Mithali\" (Konkona) is in search for from the movies beginning. \"Prof.Anu\" (Shabhana Azmi)is Mithi\\'s extremely caring and loving half sister from Mithi\\'s mom\\'s earlier marriage. The movie revolves around these characters and looks into the life of a schizophrenic patient (Mithi). The director tries to explain to the viewer the imaginary world of Mithi, through her continuous blabbering to Anu and others. <br /><br />Konkona deserves not one but thousands of awards (which I am sure, she will be getting)for this rendition of Mithi in this movie. You can see the look of a patient written on her face, by the drooping lips and sleepy eyes, from the first scene itself. Rahul Bose has done a good job, but has been reduced to one half of the movie in spite of his importance in their life.<br /><br />Watch out for the intense relationships shown between the characters of the movie, Mithi & Anu, Anu & Anu\\'s Mom and between Anu & Sanjiv (Kanwaljit Singh). Shabhana Azmi, as usual has done a riveting performance to be remembered as the sister, who sacrificed her life for Mithi.<br /><br />The movie might not be your usual Hindi potboiler, but can certainly make people look at the schizophrenic patients in a different light altogether.<br /><br />As usual, Aparna Sen brings the movie to a different ending rather than any clichéd ones, we might think off. Hats off to her, for this great movie!!!',\n       'i was disappointed in this documentary.i thought it would be about the second chess match between Grandmaster Garry Kasporov and Deep Blue the supercomputer designed by IBM computer experts to beat any human chess player.Kasparov was and still is,considered the greatest chess player ever.the movie takes us back to 1997 where Kasporov had agreed to have a rematch with \"deep Blue\" after defeating it 1 year earlier.but instead of focusing on the game,it focuses more on what happens before and after.there are snippets of the game,but not very many.much of the film centers around Kasporov\\'s paranoid obsession that the match was rigged as part of some conspiracy theory and that he lost the match unfairly.the movie also includes interviews with people who are not interesting in any way.they even chat with the manager of the building where the match took place.who cares?i also found it very dry and slow.ultimately this movie was unsatisfying.this is just my opinion,of course.if you like conspiracy theories,this movie might interest you.for people not into chess or conspiracy theories,this movie would probably have no value.i am a chess fan,and i only stuck it out because of that.i give\"Game Over:Kasparov and the Machine\" 4/10'],\n      dtype='<U12988')"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "five_errors = get_errors(customNB, ds_test, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. (the description of the mood of the movie may be considered as a spoiler - because there is not much action in fact)<br /><br />Great one...<br /><br />Is it for my peculiar interest for the dystopias and utopias? Is it for the atmosphere of the movie. Or is there some more magic? If yes, it is for sure the utmost human one...<br /><br />This film is, no doubt, extremely artistic/artificial (depends on taste). I can imagine most of the people who hate to watch slow movies (and those of Tsai Ming Liang (who I didn't enjoy other times) are one of the slowest that I know), suffering during the movie. Yes, people are unable to slow down and to let time pass - and to watch it without feeling they waste it. One can take this piece as torture or as a therapy...<br /><br />The topic at the surface? The lack of communication - even if we live in rabbit cages - one next to each other - but not really together? People are tired, sick of something and unable to describe it - just don't want to meet, touch, talk, confront the others... like if they had disappeared. The big block of flats looks void and the rain falling constantly evokes the strange melancholy inside. And sometimes it must be something abnormal, unexpected, some unwanted decay as a hole in the floor of concrete - that allows us to reach each other.<br /><br />One of the possible ways to look at it is this: Don't survey the inner world of the characters - consider the whole movie-space to be inside of yourself. And ask - why is it there? Where could these depressive states and moods come form? Is there a place for them, they don't have a right to be here? And search for the answers (if you need them) among the walls and halls of the block - instead of inside hardly transparent mind of a man.<br /><br />The key to understand is not-to-understand - to let a movie borrow us - as a subject of study - inside itself - and at the end safely return us to our more colorful and \"normal\" looking reality.<br /><br />Then, maybe, you will reach - like me - the feeling of real, possible, non-pathetic hope, that in core we are still humans... and this state of mind can help one much to live in this world.\n",
      "2. The trailer to this film focused so much on the chain (of course, because it's so sensational) that it missed most of the movie, which is about a developing, although rather simply drawn, relationship between Lazarus and Rae as they attempt to recover from their past pains with each other.<br /><br />Of course, with the premise of a nymphomaniac in chains, it's no surprise that there's plenty of implied sex involved. However, at it's core, Black Snake Moan is a basic tale of redemption and the healing power of helping another person along. Maybe it's just me though, but I think poor Lazarus should've had his story focused on more. He's a hurting man after his wife leaves him, but we never fully see how helping Rae resolve her past pains heals him too. It's just implied that it does--in essence, he plays the wizard that helps the young Rae overcome her curse, through a big ol' chain and some blues.<br /><br />I like the story, but I wish it were a bit more even and didn't have to rely on the sensational. The side characters were fairly decent, if simple and I liked the music. The acting was good enough, although I can't be certain if the Rae character is fully believable. But that might just be my naivety.<br /><br />All in all, I liked the film, but I wasn't compelled by it. Maybe it's that I'm too critical, but the story seems a little too convenient to be fully believable and so, while it all seemed very cool, I could never truly buy it. The chain thing was a little too far-fetched for me. Still, this can provide some entertainment for those looking for dramatic redemption stories with a shot of the blues. 7/10\n",
      "3. I haven't written a review on here in ages but rewatching all of bottom TV show, live shows and this I felt I had to make my views on this movie known! It is, I feel, the perfect comedy movie. It lacks the lovey dovey story lines(I wouldn't really call richies enfatuation with Gina Carbonara love would you? Or him and eddie going up there naked... not love) that make the rest of comedys go from good to crap, it lacks the usual dilemmas that one must overcome in most other comedy movies... unless you count the fact that they poisoned the guests and must escape from the guests green vomit as a dilemma thats similar to other comedy movies..... No, this movie just sets out and succeeds in doing one thing AND ONE thing only: Making one laugh. What does one require from comedy movies? Laughter. This movie just piles on laugh after laugh without stuffing up the laughs with serious crap like other comedy movies! Thus I call it the, so far, only perfect comedy movie ever made and I will never ever stop watching this beautiful movie! I appluad rick and ade on such fantastic genius!\n",
      "4. 15 PARK AVENUE is the address \"Mithi/Mithali\" (Konkona) is in search for from the movies beginning. \"Prof.Anu\" (Shabhana Azmi)is Mithi's extremely caring and loving half sister from Mithi's mom's earlier marriage. The movie revolves around these characters and looks into the life of a schizophrenic patient (Mithi). The director tries to explain to the viewer the imaginary world of Mithi, through her continuous blabbering to Anu and others. <br /><br />Konkona deserves not one but thousands of awards (which I am sure, she will be getting)for this rendition of Mithi in this movie. You can see the look of a patient written on her face, by the drooping lips and sleepy eyes, from the first scene itself. Rahul Bose has done a good job, but has been reduced to one half of the movie in spite of his importance in their life.<br /><br />Watch out for the intense relationships shown between the characters of the movie, Mithi & Anu, Anu & Anu's Mom and between Anu & Sanjiv (Kanwaljit Singh). Shabhana Azmi, as usual has done a riveting performance to be remembered as the sister, who sacrificed her life for Mithi.<br /><br />The movie might not be your usual Hindi potboiler, but can certainly make people look at the schizophrenic patients in a different light altogether.<br /><br />As usual, Aparna Sen brings the movie to a different ending rather than any clichéd ones, we might think off. Hats off to her, for this great movie!!!\n",
      "5. i was disappointed in this documentary.i thought it would be about the second chess match between Grandmaster Garry Kasporov and Deep Blue the supercomputer designed by IBM computer experts to beat any human chess player.Kasparov was and still is,considered the greatest chess player ever.the movie takes us back to 1997 where Kasporov had agreed to have a rematch with \"deep Blue\" after defeating it 1 year earlier.but instead of focusing on the game,it focuses more on what happens before and after.there are snippets of the game,but not very many.much of the film centers around Kasporov's paranoid obsession that the match was rigged as part of some conspiracy theory and that he lost the match unfairly.the movie also includes interviews with people who are not interesting in any way.they even chat with the manager of the building where the match took place.who cares?i also found it very dry and slow.ultimately this movie was unsatisfying.this is just my opinion,of course.if you like conspiracy theories,this movie might interest you.for people not into chess or conspiracy theories,this movie would probably have no value.i am a chess fan,and i only stuck it out because of that.i give\"Game Over:Kasparov and the Machine\" 4/10\n"
     ]
    }
   ],
   "source": [
    "for i, error in enumerate(five_errors):\n",
    "    print(f\"{i + 1}. {error}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysis\n",
    "\n",
    "We will interest in the 1st and 5th documents.\n",
    "\n",
    "In the first one, while the author enjoyed the movie, it also understands why other people would dislike it, and explains so.\n",
    "The model got stuck on sentence like \"most of the people who hate to watch slow movies\", without understanding that the dark mood of the movie actually resonates with the reviewer.\n",
    "This is therefore a false negative\n",
    "\n",
    "The fifth one is a contrario a false positive. The author is actually let down by the movie, which according to him does not do justice to the match between the *greatest* chess player Kasparov.\n",
    "The model misunderstanding all the superlatives to be about the film, while in reality the reviwever explains why he is let down.\n",
    "\n",
    "In both case, we have respectively bad and good words used in a good and bad context, which tricks the models as it does not have a notion of context."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FastText"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset conversion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing(doc : str) -> str:\n",
    "    \"\"\"\n",
    "    Lowers and remove punctuations, as asked in subject.\n",
    "    :param doc: Doc to preprocess.\n",
    "    :return: doc in lowercase and without punctuation.\n",
    "    \"\"\"\n",
    "    # Found the regex below to remove all punctuation in a string. Neat!\n",
    "    return re.sub(r'[^\\w\\s]', '', doc.lower())\n",
    "\n",
    "def DS_to_FastText(ds : Dataset) -> str:\n",
    "    \"\"\"\n",
    "    Converts the HuggingFace DataSet to a file comprehensible by FastText.\n",
    "    :param ds: The DataSet object to convert.\n",
    "    :return: A single str, where each line is a doc prefixed by the labelled class.\n",
    "    \"\"\"\n",
    "    labels = ds.features['label'].names\n",
    "    converted_lines = [f\"__label__{labels[doc['label']]} {preprocessing(doc['text'])}\" for doc in ds]\n",
    "    return '\\n'.join(converted_lines)\n",
    "\n",
    "def write_DS_as_FastText(ds : Dataset, path : str) -> None:\n",
    "    \"\"\"\n",
    "    Uses DS_to_FastText() to convert DataSet then write to a file.\n",
    "    :param ds: The DataSet object to convert.\n",
    "    :param path: Where the file should be saved.\n",
    "    \"\"\"\n",
    "    f = open(path, \"wb\")\n",
    "    f.write(DS_to_FastText(ds.shuffle()).encode('utf-8'))\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a\\cache-faf8c2d959925f7e.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a\\cache-e3a051aef336c446.arrow\n"
     ]
    }
   ],
   "source": [
    "# Converting and writing dataset\n",
    "PATH_FASTTEXT_FULL_TRAIN = \"datasets/full_train.txt\"\n",
    "PATH_FASTTEXT_FULL_TEST = \"datasets/full_test.txt\"\n",
    "\n",
    "write_DS_as_FastText(ds_train, PATH_FASTTEXT_FULL_TRAIN)\n",
    "write_DS_as_FastText(ds_test, PATH_FASTTEXT_FULL_TEST)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "#### With default hypers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from fasttext import FastText\n",
    "\n",
    "defaultFT = FastText.train_supervised(input = PATH_FASTTEXT_FULL_TRAIN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.87644\n"
     ]
    }
   ],
   "source": [
    "defaultFT_acc = defaultFT.test(PATH_FASTTEXT_FULL_TEST)[1]\n",
    "print(\"Accuracy :\", defaultFT_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hypers search functionality\n",
    "\n",
    "We will first need to split the dataset. Since I already have a function converting the dataset to a text file compatible with FastText, I will not use sklearn's function, but simply use HuggingGace's Slicing API."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Reusing dataset imdb (C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a\\cache-14602485a6708051.arrow\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a\\cache-38cd901810cc03d5.arrow\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "PATH_FASTTEXT_SPLIT_TRAIN = \"datasets/split_train.txt\"\n",
    "PATH_FASTTEXT_SPLIT_VALID = \"datasets/split_valid.txt\"\n",
    "VALID_SPLIT_PCT = 30\n",
    "\n",
    "# Actual splitting\n",
    "threshold = VALID_SPLIT_PCT // 2\n",
    "ds_split_valid = load_dataset('imdb', split=f'train[:{threshold}%]+train[-{threshold}%:]') # Taking validation set in the extremities.\n",
    "ds_split_train = load_dataset('imdb', split=f'train[{threshold}%:-{threshold}%]') # Train set is the remaining middle part.\n",
    "\n",
    "# Dataset are already shuffled inside the conversion function.\n",
    "write_DS_as_FastText(ds_split_train, PATH_FASTTEXT_SPLIT_TRAIN)\n",
    "write_DS_as_FastText(ds_split_valid, PATH_FASTTEXT_SPLIT_VALID)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Search\n",
    "hyperFT = FastText.train_supervised(input=PATH_FASTTEXT_SPLIT_TRAIN, autotuneValidationFile=PATH_FASTTEXT_SPLIT_VALID)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.88776\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "hyperFT_acc = hyperFT.test(PATH_FASTTEXT_FULL_TEST)[1]\n",
    "print(\"Accuracy :\", hyperFT_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model differences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Attributes\n",
    "\n",
    "def extract_attributes(model : FastText) -> str:\n",
    "    return f\"\"\"attributes :\n",
    "  * bucket : {model.bucket}\n",
    "  * dim : {model.dim}\n",
    "  * lr : {model.lr}\n",
    "  * lrUpdateRate : {model.lrUpdateRate}\n",
    "  * maxn : {model.maxn}\n",
    "  * minn : {model.minn}\n",
    "  * wordNgrams : {model.wordNgrams}\n",
    "  * ws : {model.ws}\n",
    "  * loss : {model.loss}\n",
    "  * minCount : {model.minCount}\n",
    "  * minCountLabel : {model.minCountLabel}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default model attributes :\n",
      "  * bucket : 0\n",
      "  * dim : 100\n",
      "  * lr : 0.1\n",
      "  * lrUpdateRate : 100\n",
      "  * maxn : 0\n",
      "  * minn : 0\n",
      "  * wordNgrams : 1\n",
      "  * ws : 5\n",
      "  * loss : loss_name.softmax\n",
      "  * minCount : 1\n",
      "  * minCountLabel : 0\n",
      "\n",
      "Hypertuned model attributes :\n",
      "  * bucket : 2797238\n",
      "  * dim : 10\n",
      "  * lr : 5.0\n",
      "  * lrUpdateRate : 100\n",
      "  * maxn : 0\n",
      "  * minn : 0\n",
      "  * wordNgrams : 3\n",
      "  * ws : 5\n",
      "  * loss : loss_name.softmax\n",
      "  * minCount : 1\n",
      "  * minCountLabel : 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Default model\", extract_attributes(defaultFT))\n",
    "print(\"Hypertuned model\", extract_attributes(hyperFT))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysis :\n",
    "* The hypertuned model is way more aggressive in its learning rate (quintuple the default one)\n",
    "* Its dimension however, is reduced by an order of magnitude (100 vs 9)\n",
    "* The hypertuned model work using trigrams, which was hypothetised when workign with the NaiveBayes model above.\n",
    "* minn and maxn are both nullified in both models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Digging up errors\n",
    "\n",
    "In order to compare the models, we will use the 5 same sentences as in the Naive Bayesian model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "tricky_labels_pred = hyperFT.predict([preprocessing(err) for err in five_errors])[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. NEG : the description of the mood of the movie may be considered as a spoiler  because there is not much action in factbr br great onebr br is it for my peculiar interest for the dystopias and utopias is it for the atmosphere of the movie or is there some more magic if yes it is for sure the utmost human onebr br this film is no doubt extremely artisticartificial depends on taste i can imagine most of the people who hate to watch slow movies and those of tsai ming liang who i didnt enjoy other times are one of the slowest that i know suffering during the movie yes people are unable to slow down and to let time pass  and to watch it without feeling they waste it one can take this piece as torture or as a therapybr br the topic at the surface the lack of communication  even if we live in rabbit cages  one next to each other  but not really together people are tired sick of something and unable to describe it  just dont want to meet touch talk confront the others like if they had disappeared the big block of flats looks void and the rain falling constantly evokes the strange melancholy inside and sometimes it must be something abnormal unexpected some unwanted decay as a hole in the floor of concrete  that allows us to reach each otherbr br one of the possible ways to look at it is this dont survey the inner world of the characters  consider the whole moviespace to be inside of yourself and ask  why is it there where could these depressive states and moods come form is there a place for them they dont have a right to be here and search for the answers if you need them among the walls and halls of the block  instead of inside hardly transparent mind of a manbr br the key to understand is nottounderstand  to let a movie borrow us  as a subject of study  inside itself  and at the end safely return us to our more colorful and normal looking realitybr br then maybe you will reach  like me  the feeling of real possible nonpathetic hope that in core we are still humans and this state of mind can help one much to live in this world\n",
      "2. POS : the trailer to this film focused so much on the chain of course because its so sensational that it missed most of the movie which is about a developing although rather simply drawn relationship between lazarus and rae as they attempt to recover from their past pains with each otherbr br of course with the premise of a nymphomaniac in chains its no surprise that theres plenty of implied sex involved however at its core black snake moan is a basic tale of redemption and the healing power of helping another person along maybe its just me though but i think poor lazarus shouldve had his story focused on more hes a hurting man after his wife leaves him but we never fully see how helping rae resolve her past pains heals him too its just implied that it doesin essence he plays the wizard that helps the young rae overcome her curse through a big ol chain and some bluesbr br i like the story but i wish it were a bit more even and didnt have to rely on the sensational the side characters were fairly decent if simple and i liked the music the acting was good enough although i cant be certain if the rae character is fully believable but that might just be my naivetybr br all in all i liked the film but i wasnt compelled by it maybe its that im too critical but the story seems a little too convenient to be fully believable and so while it all seemed very cool i could never truly buy it the chain thing was a little too farfetched for me still this can provide some entertainment for those looking for dramatic redemption stories with a shot of the blues 710\n",
      "3. POS : i havent written a review on here in ages but rewatching all of bottom tv show live shows and this i felt i had to make my views on this movie known it is i feel the perfect comedy movie it lacks the lovey dovey story linesi wouldnt really call richies enfatuation with gina carbonara love would you or him and eddie going up there naked not love that make the rest of comedys go from good to crap it lacks the usual dilemmas that one must overcome in most other comedy movies unless you count the fact that they poisoned the guests and must escape from the guests green vomit as a dilemma thats similar to other comedy movies no this movie just sets out and succeeds in doing one thing and one thing only making one laugh what does one require from comedy movies laughter this movie just piles on laugh after laugh without stuffing up the laughs with serious crap like other comedy movies thus i call it the so far only perfect comedy movie ever made and i will never ever stop watching this beautiful movie i appluad rick and ade on such fantastic genius\n",
      "4. POS : 15 park avenue is the address mithimithali konkona is in search for from the movies beginning profanu shabhana azmiis mithis extremely caring and loving half sister from mithis moms earlier marriage the movie revolves around these characters and looks into the life of a schizophrenic patient mithi the director tries to explain to the viewer the imaginary world of mithi through her continuous blabbering to anu and others br br konkona deserves not one but thousands of awards which i am sure she will be gettingfor this rendition of mithi in this movie you can see the look of a patient written on her face by the drooping lips and sleepy eyes from the first scene itself rahul bose has done a good job but has been reduced to one half of the movie in spite of his importance in their lifebr br watch out for the intense relationships shown between the characters of the movie mithi  anu anu  anus mom and between anu  sanjiv kanwaljit singh shabhana azmi as usual has done a riveting performance to be remembered as the sister who sacrificed her life for mithibr br the movie might not be your usual hindi potboiler but can certainly make people look at the schizophrenic patients in a different light altogetherbr br as usual aparna sen brings the movie to a different ending rather than any clichéd ones we might think off hats off to her for this great movie\n",
      "5. NEG : i was disappointed in this documentaryi thought it would be about the second chess match between grandmaster garry kasporov and deep blue the supercomputer designed by ibm computer experts to beat any human chess playerkasparov was and still isconsidered the greatest chess player everthe movie takes us back to 1997 where kasporov had agreed to have a rematch with deep blue after defeating it 1 year earlierbut instead of focusing on the gameit focuses more on what happens before and afterthere are snippets of the gamebut not very manymuch of the film centers around kasporovs paranoid obsession that the match was rigged as part of some conspiracy theory and that he lost the match unfairlythe movie also includes interviews with people who are not interesting in any waythey even chat with the manager of the building where the match took placewho caresi also found it very dry and slowultimately this movie was unsatisfyingthis is just my opinionof courseif you like conspiracy theoriesthis movie might interest youfor people not into chess or conspiracy theoriesthis movie would probably have no valuei am a chess fanand i only stuck it out because of thati givegame overkasparov and the machine 410\n"
     ]
    }
   ],
   "source": [
    "for i, (error, label) in enumerate(list(zip(five_errors, tricky_labels_pred))):\n",
    "    print(f\"{i + 1}. {label[0][9:].upper()} : {preprocessing(error)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysis :\n",
    "\n",
    "We see that most of the tricky cases failed by the Naive Bayesian model were correctly guessed by FastText.\n",
    "However, it still got lead astray int he first one by the explanation of how the reviewer may understand why *others* may have disliked the movie (but how he personally enjoyed it nonetheless).\n",
    "\n",
    "To be fair to the model this example can be considered tricky, as the reviewer employs many subjonctives, so tracking context is difficult. Even more so that author says how the movie touched him, but did not say outright that he *liked* it (as human, we have the inner knowledge that art has the goal of resonating, one way or another, with the observer. However again, FastText lacks such context). \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MINN and MAXN nullified ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysis\n",
    "\n",
    "#TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Theoretical questions\n",
    "\n",
    "Answer the following questions.\n",
    "\n",
    "1. Explain with your own words, using a short paragraph for each, what are:\n",
    "   * Phonetics and phonology\n",
    "   * Morphology and syntax\n",
    "   * Semantics and pragmatics\n",
    "\n",
    "```\n",
    "Answer\n",
    "```\n",
    "\n",
    "2. What is the difference between stemming and lemmatization?\n",
    "   * How do they both work?\n",
    "   * What are the pros and cons of both methods?\n",
    "\n",
    "```\n",
    "Answer\n",
    "```\n",
    "\n",
    "3. On logistic regression:\n",
    "   * How does stochastic gradient descent work?\n",
    "   * What is the role of the learning rate?\n",
    "   * Will it always find the global minimum?\n",
    "\n",
    "```\n",
    "Answer\n",
    "```\n",
    "\n",
    "4. What problems does TF-iDF try to solve?\n",
    "   * What the is the TF part for?\n",
    "   * What is the iDF part for?\n",
    "\n",
    "```\n",
    "Answer\n",
    "```\n",
    "\n",
    "5. Summarize how the skip-gram method of Word2Vec works using a couple of paragraphs.\n",
    "   * How does it uses the fact that _two words appearing in similar contexts are likely to have similar meanings_?\n",
    "\n",
    "```\n",
    "Answer\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}