{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to Natural Language Processing Catch-up 1\n",
    " \n",
    "Authors :\n",
    " * Tony George"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "*(Copied from subject)*\n",
    "\n",
    "In this small project, you will code a sentiment classifier using the naive Bayes methods seen in class and compare it with the FastText library. There are a few theoritical questions to answer as well.\n",
    "\n",
    "Please, read the full assignment before starting.\n",
    "\n",
    "For coding standards, please respect the following guidelines\n",
    "\n",
    "* Use docstring format to describe your functions and their arguments.\n",
    "* Use typing.\n",
    "* Have clear and verbatim variable names (not x, x1, x2, xx, another_x, ...).\n",
    "* Make your results reproducible (force random seeds values when necessary and possible).\n",
    "* Don't hesitate commenting in details part of the code you consider complex or hard to read.\n",
    "\n",
    "Do not hesitate contacting me if you have any question, but please don't wait until the last moment to do so."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports\n",
    "\n",
    "Using conda with python version 3.8. A conda yml should be available if I didn't forget to generate it.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "# TODO : Generate conda yml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Forcing seed\n",
    "\n",
    "This helps reproducibility, feel free to play with !"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The dataset\n",
    "\n",
    "Using HuggingFace's version of the IMDB dataset as asked by subject.\n",
    "\n",
    "### Importing from HuggingFace"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desc : Large Movie Review Dataset.\n",
      "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
      "Features : {'text': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['neg', 'pos'], names_file=None, id=None)}\n",
      "Splits : {'train': SplitInfo(name='train', num_bytes=33432835, num_examples=25000, dataset_name='imdb'), 'test': SplitInfo(name='test', num_bytes=32650697, num_examples=25000, dataset_name='imdb'), 'unsupervised': SplitInfo(name='unsupervised', num_bytes=67106814, num_examples=50000, dataset_name='imdb')}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_dataset_builder\n",
    "\n",
    "ds_info = load_dataset_builder(\"imdb\")\n",
    "print(\"Desc :\", ds_info.info.description)\n",
    "print(\"Features :\", ds_info.info.features)\n",
    "print(\"Splits :\", ds_info.info.splits)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n",
      "Reusing dataset imdb (C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    }
   ],
   "source": [
    "# Cheating a bit by downloading the DS now\n",
    "ds_train    = load_dataset(\"imdb\", split = \"train\")\n",
    "ds_test     = load_dataset(\"imdb\", split = \"test\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset Exploration\n",
    "\n",
    "#### How many splits does the dataset has?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['train', 'test', 'unsupervised'])"
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the builder to get info on splits.\n",
    "ds_info.info.splits.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "HuggingFaces's version of the IMDB dataset has 3 different splits, though we will only interest ourselves in the first two ones.\n",
    "\n",
    "The *train* and *test* split are your standard ML splits, while the *unsupervised* data contains unlabelled data for those willing to gain more volumetry at the cost of some work."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### How big are these splits?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "data": {
      "text/plain": "['Split train contains 25000 examples (33.4 MB).',\n 'Split test contains 25000 examples (32.7 MB).',\n 'Split unsupervised contains 50000 examples (67.1 MB).']"
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This small lib is great to convert hard numbers into a human-readable format.\n",
    "from humanize import naturalsize\n",
    "from datasets import SplitInfo\n",
    "\n",
    "def split_info_on(sp_info : SplitInfo) -> str:\n",
    "    \"\"\"\n",
    "    :param sp_info: The SplitInfo object from HuggingFace's datasets lib.\n",
    "    :return: Human readable string with a quick description on the object.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Split {sp_info.name} contains {sp_info.num_examples} examples ({naturalsize(sp_info.num_bytes)}).\"\"\"\n",
    "\n",
    "# Coming from Scala, having to wrap a map object in Python makes me sad.\n",
    "# You will further down that my mindset is heavily ~~~corrupted~~~ inspired by MapReduce.\n",
    "list(map(split_info_on, ds_info.info.splits.values()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In term of size, we have a 50/50 split between labelled and unlabelled data, and another 50/50 between the *train* and *test* split for the former one."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### What is the proportion of each class on the supervised splits?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from typing import Dict\n",
    "\n",
    "def class_histogram(ds : Dataset) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    :param ds: Dataset object to dress the class histogram of.\n",
    "    :return: List of Tuple2 [('class_label', count), ...]\n",
    "    \"\"\"\n",
    "    labels = ds.info.features['label'].names\n",
    "    # Extracting the label from the example, then counting occurrences using np.unique()\n",
    "    _, counts = np.unique(ds['label'], return_counts = True)\n",
    "    return dict(zip(labels, counts))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train : {'neg': 12500, 'pos': 12500}\n",
      "Test : {'neg': 12500, 'pos': 12500}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train :\", class_histogram(ds_train))\n",
    "print(\"Test :\", class_histogram(ds_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both our splits have no bias toward a class or the other, with again a perfect  50/50 split."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes classifier\n",
    "\n",
    "### Pretreatment\n",
    "\n",
    "Using the standard NLP pretreatment workflow, up to stemming.\n",
    "\n",
    "The reason I don't do lemmatization is because my pretreatment function is simple and tends to butcher words.\n",
    "(i.e. \"you'll\" becomes [\"you\", \"'ll\"], and wordnet lemmatizer does not recognize the second word as will).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Griffures\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Griffures\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Downloading NLTK (we will use it below anyway).\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stops_en = set(stopwords.words('english'))\n",
    "lancaster = LancasterStemmer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "outputs": [],
   "source": [
    "def pretreatment(text : str) -> str:\n",
    "    # Lowercasing\n",
    "    raw_words = word_tokenize(text.lower())\n",
    "\n",
    "    # Filter out : punctuation & stop stopwords\n",
    "    is_relevant = lambda word : not (all(map(lambda c: c in punctuation, list(word))) # Removing punctuation only words\n",
    "                                     or word in stops_en) # BONUS : Removing stopwords\n",
    "    filtered_words = list(filter(is_relevant, raw_words))\n",
    "\n",
    "    # BONUS : Stemming\n",
    "    lemmatized_words = list(map(lancaster.stem, filtered_words))\n",
    "\n",
    "    return  ' '.join(lemmatized_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example with : I went and saw this movie last night after being coaxed to by a few friends of mine. I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. The sign of a good movie is that it can toy with our emotions. This one did exactly that. The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. This movie was great, and I suggest that you go see it before you judge.\n",
      "Which gives : went saw movy last night coax friend min 'll admit reluct see knew ashton kutch abl comedy wrong kutch play charact jak fisch wel kevin costn play ben randal profess sign good movy toy emot on exact entir the sold overcom laught first half movy mov tear second half exit the saw many wom tear many ful grown men wel try desp let anyon see cry movy gre suggest go see judg\n"
     ]
    }
   ],
   "source": [
    "# Select an example here\n",
    "ex_preprocessing = ds_test[0]['text']\n",
    "\n",
    "print(\"Example with :\", ex_preprocessing)\n",
    "print(\"Which gives :\", pretreatment(ex_preprocessing))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implementing the model\n",
    "\n",
    "As to not rely too much on sklearn's library, and only using it to gain time,\n",
    "we will override the preprocessing function of the CountVectorizer with our own, and disable its stopwords list.\n",
    "\n",
    "The model will actually take a hit from this workflow (both in performance, as the tokenizing will actually happen twice, and in accuracy, as we could do everything with a well configured CountVectorizer).\n",
    "However, it proves a greater comprehension of the notions at play."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy.typing as npt\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class CustomNaiveBayes:\n",
    "    \"\"\"\n",
    "    Wrapper class to articulate sklearn's CountVectorizer and MultinomialNB models.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(preprocessor = pretreatment, stop_words = None)\n",
    "    clf = MultinomialNB()\n",
    "    labels = []\n",
    "\n",
    "    def fit(self, ds : Dataset) -> None:\n",
    "        \"\"\"\n",
    "        Wrapper function to launch a complete train workflow from a HF's dataset\n",
    "        :param ds: HuggingFace DataSet object to train on.\n",
    "        :return: Nothing.\n",
    "        \"\"\"\n",
    "        # Extracting labels for predict_label function\n",
    "        self.labels = ds.features['label'].names\n",
    "        # Shuffling and extracting features\n",
    "        ds.shuffle()\n",
    "        X_as_docs, y_as_ints = self.DS_to_XnY(ds)\n",
    "        X_as_features = self.vectorizer.fit_transform(X_as_docs)\n",
    "        # Actual training\n",
    "        self.clf.fit(X_as_features, y_as_ints)\n",
    "\n",
    "    def predict(self, X_as_docs : npt.ArrayLike) -> npt.NDArray[int]:\n",
    "        \"\"\"\n",
    "        Extract features then launch model's predictions on a list of documents\n",
    "        :param X_as_docs: List of documents (corpus)\n",
    "        :return: Predicted classes as ints\n",
    "        \"\"\"\n",
    "        X_as_features = self.vectorizer.transform(X_as_docs)\n",
    "        return self.clf.predict(X_as_features)\n",
    "\n",
    "    def predict_label(self, X_as_docs : npt.ArrayLike) -> npt.NDArray[str]:\n",
    "        \"\"\"\n",
    "        Same as predict, but return the classes label\n",
    "        :param X_as_docs: List of documents (corpus)\n",
    "        :return: Predicted classes as strings\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X_as_docs)\n",
    "        return np.fromiter([self.labels[y] for y in predictions], str)\n",
    "\n",
    "    def score(self, ds : Dataset) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the model with a HF's dataset.\n",
    "        :param ds: HuggingFace DataSet object to evaluate upon.\n",
    "        :return: Accuracy as float (0~1).\n",
    "        \"\"\"\n",
    "        X_as_docs, y_as_ints = self.DS_to_XnY(ds)\n",
    "        X_as_features = self.vectorizer.transform(X_as_docs)\n",
    "        return self.clf.score(X_as_features, y_as_ints)\n",
    "\n",
    "    def DS_to_XnY(self, ds : Dataset) -> Tuple[npt.ArrayLike, npt.ArrayLike]:\n",
    "        \"\"\"\n",
    "        Transforms a HuggingFace dataset object to a more usual X and y tuple for fitting.\n",
    "        :param ds: Dataset to convert\n",
    "        :return: Tuple : X as list[str] and Y as list[int], both of the same len.\n",
    "        \"\"\"\n",
    "        raw_corpus = np.array(ds['text'])\n",
    "        targets = np.array(ds['label'])\n",
    "        return raw_corpus, targets\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\Griffures\\.cache\\huggingface\\datasets\\imdb\\plain_text\\1.0.0\\e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a\\cache-9d17e850e5680b7a.arrow\n"
     ]
    }
   ],
   "source": [
    "customNB = CustomNaiveBayes()\n",
    "customNB.fit(ds_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reporting accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.81168\n"
     ]
    }
   ],
   "source": [
    "customNB_acc = customNB.score(ds_test)\n",
    "print(\"Accuracy :\", customNB_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Is accuracy it sufficient here ?\n",
    "\n",
    "Accuracy is a sufficient metric here for multiple reasons :\n",
    "\n",
    "* Since we only have 2 classes, a confusion matrice looses its relevance (p versus 1 - p).\n",
    "* Due to the nature of our classification, we do not care specifically for type 1 nor type 2 errors. Therefore focusing Precision or Recall is meaningless.\n",
    "* The above, plus the fact that our classes are perfectly balanced, makes f1-score a very similar metric as accuracy (harmonic mean between precision and recall)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Top 10 important feature from each class\n",
    "\n",
    "I already removed the stop words, so we already have the best results ! :)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [],
   "source": [
    "def top_10_features_per_class(model: CustomNaiveBayes) -> Dict:\n",
    "    \"\"\"\n",
    "    Return the top ten heaviest features for each class according to the model\n",
    "    :param model: the model to extract weights from\n",
    "    :return: Dictionnary : Class Label -> Features (feature(stem) -> log_value)\n",
    "    \"\"\"\n",
    "    # Using a custom dtype to preserve info on features' names.\n",
    "    feature_log_t = [(\"feature\", '<U50'), (\"log_value\", float)]\n",
    "    # Just stacks the features names (stems) on top of the logs array\n",
    "    feature_stack = lambda logs : np.array(list(zip(model.vectorizer.get_feature_names_out(), logs)), dtype = feature_log_t)\n",
    "    # Sort the array, take the 10 last value, then flip it to get our top 10\n",
    "    sort_10_biggest = lambda arr : np.flip(np.sort(arr, order=\"log_value\")[-10:])\n",
    "\n",
    "    # Applying the functions in the correct order\n",
    "    logs_with_features = [sort_10_biggest(feature_stack(logs)) for logs in model.clf.feature_log_prob_]\n",
    "\n",
    "    return dict(zip(model.labels, map(dict, logs_with_features)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features (stems) in NEG :\n",
      "  * br        -3.41\n",
      "  * movy      -4.04\n",
      "  * film      -4.25\n",
      "  * on        -4.78\n",
      "  * act       -4.79\n",
      "  * lik       -4.87\n",
      "  * ev        -4.89\n",
      "  * real      -5.09\n",
      "  * mak       -5.27\n",
      "  * bad       -5.3\n",
      "Top 10 features (stems) in POS :\n",
      "  * br        -3.51\n",
      "  * film      -4.18\n",
      "  * movy      -4.32\n",
      "  * on        -4.76\n",
      "  * act       -5.05\n",
      "  * lik       -5.06\n",
      "  * real      -5.09\n",
      "  * ev        -5.2\n",
      "  * tim       -5.31\n",
      "  * good      -5.35\n"
     ]
    }
   ],
   "source": [
    "# Simple extraction of the above dictionary\n",
    "for label, features in top_10_features_per_class(customNB).items():\n",
    "    print(f\"Top 10 features (stems) in {label.upper()} :\")\n",
    "    print('\\n'.join([f\"  * {feature:10}{log:2.3}\" for feature, log in features.items()]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysis :\n",
    "\n",
    "Prevalent stems common in both classes (8/10) :\n",
    "* br\n",
    "* movy\n",
    "* film\n",
    "* on\n",
    "* act\n",
    "* like\n",
    "* real\n",
    "* ev\n",
    "\n",
    "Prevalent NEG stems :\n",
    "* mak\n",
    "* bad\n",
    "\n",
    "Prevalent POS stems :\n",
    "* time\n",
    "* good\n",
    "\n",
    "A great number of steams are common between our two classes. This makes me think that a greater notion of context should be given to better the model.\n",
    "Just giving the CountVectorizer 2-grams would help it makes the difference between 'bad' and 'not bad', 'good act' and 'bad act', etc.\n",
    "\n",
    "However 81% for such a simple model is not bad at all already ! (n-grams would explode the size of our vocabulary)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Digging up errors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [],
   "source": [
    "def get_errors(model : CustomNaiveBayes, ds : Dataset, n):\n",
    "    X_as_docs, Y = model.DS_to_XnY(ds.shuffle()[n * 100:])\n",
    "    y_pred = model.predict(X_as_docs)\n",
    "    errors = X_as_docs[y_pred != Y]\n",
    "    return errors[:n]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. (the description of the mood of the movie may be considered as a spoiler - because there is not much action in fact)<br /><br />Great one...<br /><br />Is it for my peculiar interest for the dystopias and utopias? Is it for the atmosphere of the movie. Or is there some more magic? If yes, it is for sure the utmost human one...<br /><br />This film is, no doubt, extremely artistic/artificial (depends on taste). I can imagine most of the people who hate to watch slow movies (and those of Tsai Ming Liang (who I didn't enjoy other times) are one of the slowest that I know), suffering during the movie. Yes, people are unable to slow down and to let time pass - and to watch it without feeling they waste it. One can take this piece as torture or as a therapy...<br /><br />The topic at the surface? The lack of communication - even if we live in rabbit cages - one next to each other - but not really together? People are tired, sick of something and unable to describe it - just don't want to meet, touch, talk, confront the others... like if they had disappeared. The big block of flats looks void and the rain falling constantly evokes the strange melancholy inside. And sometimes it must be something abnormal, unexpected, some unwanted decay as a hole in the floor of concrete - that allows us to reach each other.<br /><br />One of the possible ways to look at it is this: Don't survey the inner world of the characters - consider the whole movie-space to be inside of yourself. And ask - why is it there? Where could these depressive states and moods come form? Is there a place for them, they don't have a right to be here? And search for the answers (if you need them) among the walls and halls of the block - instead of inside hardly transparent mind of a man.<br /><br />The key to understand is not-to-understand - to let a movie borrow us - as a subject of study - inside itself - and at the end safely return us to our more colorful and \"normal\" looking reality.<br /><br />Then, maybe, you will reach - like me - the feeling of real, possible, non-pathetic hope, that in core we are still humans... and this state of mind can help one much to live in this world.\n",
      "2. The trailer to this film focused so much on the chain (of course, because it's so sensational) that it missed most of the movie, which is about a developing, although rather simply drawn, relationship between Lazarus and Rae as they attempt to recover from their past pains with each other.<br /><br />Of course, with the premise of a nymphomaniac in chains, it's no surprise that there's plenty of implied sex involved. However, at it's core, Black Snake Moan is a basic tale of redemption and the healing power of helping another person along. Maybe it's just me though, but I think poor Lazarus should've had his story focused on more. He's a hurting man after his wife leaves him, but we never fully see how helping Rae resolve her past pains heals him too. It's just implied that it does--in essence, he plays the wizard that helps the young Rae overcome her curse, through a big ol' chain and some blues.<br /><br />I like the story, but I wish it were a bit more even and didn't have to rely on the sensational. The side characters were fairly decent, if simple and I liked the music. The acting was good enough, although I can't be certain if the Rae character is fully believable. But that might just be my naivety.<br /><br />All in all, I liked the film, but I wasn't compelled by it. Maybe it's that I'm too critical, but the story seems a little too convenient to be fully believable and so, while it all seemed very cool, I could never truly buy it. The chain thing was a little too far-fetched for me. Still, this can provide some entertainment for those looking for dramatic redemption stories with a shot of the blues. 7/10\n",
      "3. I haven't written a review on here in ages but rewatching all of bottom TV show, live shows and this I felt I had to make my views on this movie known! It is, I feel, the perfect comedy movie. It lacks the lovey dovey story lines(I wouldn't really call richies enfatuation with Gina Carbonara love would you? Or him and eddie going up there naked... not love) that make the rest of comedys go from good to crap, it lacks the usual dilemmas that one must overcome in most other comedy movies... unless you count the fact that they poisoned the guests and must escape from the guests green vomit as a dilemma thats similar to other comedy movies..... No, this movie just sets out and succeeds in doing one thing AND ONE thing only: Making one laugh. What does one require from comedy movies? Laughter. This movie just piles on laugh after laugh without stuffing up the laughs with serious crap like other comedy movies! Thus I call it the, so far, only perfect comedy movie ever made and I will never ever stop watching this beautiful movie! I appluad rick and ade on such fantastic genius!\n",
      "4. 15 PARK AVENUE is the address \"Mithi/Mithali\" (Konkona) is in search for from the movies beginning. \"Prof.Anu\" (Shabhana Azmi)is Mithi's extremely caring and loving half sister from Mithi's mom's earlier marriage. The movie revolves around these characters and looks into the life of a schizophrenic patient (Mithi). The director tries to explain to the viewer the imaginary world of Mithi, through her continuous blabbering to Anu and others. <br /><br />Konkona deserves not one but thousands of awards (which I am sure, she will be getting)for this rendition of Mithi in this movie. You can see the look of a patient written on her face, by the drooping lips and sleepy eyes, from the first scene itself. Rahul Bose has done a good job, but has been reduced to one half of the movie in spite of his importance in their life.<br /><br />Watch out for the intense relationships shown between the characters of the movie, Mithi & Anu, Anu & Anu's Mom and between Anu & Sanjiv (Kanwaljit Singh). Shabhana Azmi, as usual has done a riveting performance to be remembered as the sister, who sacrificed her life for Mithi.<br /><br />The movie might not be your usual Hindi potboiler, but can certainly make people look at the schizophrenic patients in a different light altogether.<br /><br />As usual, Aparna Sen brings the movie to a different ending rather than any clich√©d ones, we might think off. Hats off to her, for this great movie!!!\n",
      "5. i was disappointed in this documentary.i thought it would be about the second chess match between Grandmaster Garry Kasporov and Deep Blue the supercomputer designed by IBM computer experts to beat any human chess player.Kasparov was and still is,considered the greatest chess player ever.the movie takes us back to 1997 where Kasporov had agreed to have a rematch with \"deep Blue\" after defeating it 1 year earlier.but instead of focusing on the game,it focuses more on what happens before and after.there are snippets of the game,but not very many.much of the film centers around Kasporov's paranoid obsession that the match was rigged as part of some conspiracy theory and that he lost the match unfairly.the movie also includes interviews with people who are not interesting in any way.they even chat with the manager of the building where the match took place.who cares?i also found it very dry and slow.ultimately this movie was unsatisfying.this is just my opinion,of course.if you like conspiracy theories,this movie might interest you.for people not into chess or conspiracy theories,this movie would probably have no value.i am a chess fan,and i only stuck it out because of that.i give\"Game Over:Kasparov and the Machine\" 4/10\n"
     ]
    }
   ],
   "source": [
    "for i, error in enumerate(get_errors(customNB, ds_test, 5)):\n",
    "    print(f\"{i + 1}. {error}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysis\n",
    "\n",
    "We will interest in the 1st and 5th documents.\n",
    "\n",
    "In the first one, while the author enjoyed the movie, it also understands why other people would dislike it, and explains so.\n",
    "The model got stuck on sentence like \"most of the people who hate to watch slow movies\", without understanding that the dark mood of the movie actually resonates with the reviewer.\n",
    "This is therefore a false negative\n",
    "\n",
    "The fifth one is a contrario a false positive. The author is actually let down by the movie, which according to him does not do justice to the match between the *greatest* chess player Kasparov.\n",
    "The model misunderstanding all the superlatives to be about the film, while in reality the reviwever explains why he is let down.\n",
    "\n",
    "In both case, we have respectively bad and good words used in a good and bad context, which tricks the models as it does not have a notion of context."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## FastText"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset conversion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "# TODO"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [],
   "source": [
    "# Also modifying pretreatment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "#### With default hypers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hypers search functionality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [],
   "source": [
    "# Dataset split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "outputs": [],
   "source": [
    "# Search"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model differences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [
    "# Attributes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Digging up errors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MINN and MAXN nullified ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Theorical questions\n",
    "\n",
    "Answer the following questions.\n",
    "\n",
    "1. Explain with your own words, using a short paragraph for each, what are:\n",
    "   * Phonetics and phonology\n",
    "   * Morphology and syntax\n",
    "   * Semantics and pragmatics\n",
    "\n",
    "```\n",
    "Answer\n",
    "```\n",
    "\n",
    "2. What is the difference between stemming and lemmatization?\n",
    "   * How do they both work?\n",
    "   * What are the pros and cons of both methods?\n",
    "\n",
    "```\n",
    "Answer\n",
    "```\n",
    "\n",
    "3. On logistic regression:\n",
    "   * How does stochastic gradient descent work?\n",
    "   * What is the role of the learning rate?\n",
    "   * Will it always find the global minimum?\n",
    "\n",
    "```\n",
    "Answer\n",
    "```\n",
    "\n",
    "4. What problems does TF-iDF try to solve?\n",
    "   * What the is the TF part for?\n",
    "   * What is the iDF part for?\n",
    "\n",
    "```\n",
    "Answer\n",
    "```\n",
    "\n",
    "5. Summarize how the skip-gram method of Word2Vec works using a couple of paragraphs.\n",
    "   * How does it uses the fact that _two words appearing in similar contexts are likely to have similar meanings_?\n",
    "\n",
    "```\n",
    "Answer\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}